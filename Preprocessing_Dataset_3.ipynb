{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Step 1: Setup & Configuration"
      ],
      "metadata": {
        "id": "h0KPTdjMUfC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptNwRv3zqRP6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'train_file': 'train-2.txt',\n",
        "    'test_split': 0.2,\n",
        "    'min_user_interactions': 0,\n",
        "    'min_item_interactions': 0,\n",
        "    'random_seed': 42\n",
        "}\n",
        "\n",
        "print(\" Configuration loaded\")\n",
        "print(f\"  Train file: {CONFIG['train_file']}\")\n",
        "print(f\"  Validation split: {CONFIG['test_split']*100:.0f}%\")\n",
        "print(f\"  Random seed: {CONFIG['random_seed']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 2: Data Loading"
      ],
      "metadata": {
        "id": "s4KmlvdZUXnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_interaction_data(filepath):\n",
        "    interactions = []\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            parts = line.strip().split()\n",
        "\n",
        "            # Skip empty lines\n",
        "            if len(parts) <= 1:\n",
        "                continue\n",
        "\n",
        "            user = parts[0]\n",
        "            items = parts[1:]\n",
        "\n",
        "            # Create one row per interaction\n",
        "            for item in items:\n",
        "                interactions.append((user, item))\n",
        "\n",
        "            # Progress update every 10k lines\n",
        "            if line_num % 10000 == 0:\n",
        "                print(f\"  Processed {line_num:,} users...\", end='\\r')\n",
        "\n",
        "    print(f\"  Processed {line_num:,} users... Done!      \")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(interactions, columns=['user_raw', 'item_raw'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "inter_df = load_interaction_data(CONFIG['train_file'])\n",
        "\n",
        "print(f\"\\n Data loaded:\")\n",
        "print(f\"  Raw interactions: {len(inter_df):,}\")\n",
        "print(f\"  Unique users: {inter_df['user_raw'].nunique():,}\")\n",
        "print(f\"  Unique items: {inter_df['item_raw'].nunique():,}\")\n"
      ],
      "metadata": {
        "id": "8w3QbFC_UU0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Data Quality Check & Cleaning"
      ],
      "metadata": {
        "id": "HzMLjgQ_Yk6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for nulls\n",
        "null_count = inter_df.isnull().sum().sum()\n",
        "print(f\"\\nNull values: {null_count}\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_count = inter_df.duplicated().sum()\n",
        "print(f\"Duplicate interactions: {duplicate_count:,} ({100*duplicate_count/len(inter_df):.2f}%)\")\n",
        "\n",
        "# Store original count\n",
        "original_count = len(inter_df)\n",
        "\n",
        "# Remove duplicates\n",
        "if duplicate_count > 0:\n",
        "    print(f\"\\nRemoving duplicates...\")\n",
        "    inter_df = inter_df.drop_duplicates()\n",
        "    print(f\"  Before: {original_count:,}\")\n",
        "    print(f\"  After: {len(inter_df):,}\")\n",
        "    print(f\"  Removed: {duplicate_count:,}\")\n",
        "else:\n",
        "    print(\"\\n No duplicates found\")\n",
        "\n",
        "print(f\"\\n Data cleaned:\")\n",
        "print(f\"  Final interactions: {len(inter_df):,}\")\n",
        "print(f\"  Unique users: {inter_df['user_raw'].nunique():,}\")\n",
        "print(f\"  Unique items: {inter_df['item_raw'].nunique():,}\")\n"
      ],
      "metadata": {
        "id": "GNjkxIlPYdCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9W4cr59BZc_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 : Statistical Analysis"
      ],
      "metadata": {
        "id": "OUqCU7S6ZhvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_counts = inter_df[\"user_raw\"].value_counts()\n",
        "item_counts = inter_df[\"item_raw\"].value_counts()\n",
        "\n",
        "print(f\"\\nUsers:\")\n",
        "print(f\"  Total: {len(user_counts):,}\")\n",
        "print(f\"  Mean interactions: {user_counts.mean():.1f}\")\n",
        "print(f\"  Median: {user_counts.median():.0f}\")\n",
        "\n",
        "print(f\"\\nItems:\")\n",
        "print(f\"  Total: {len(item_counts):,}\")\n",
        "print(f\"  Mean interactions: {item_counts.mean():.1f}\")\n",
        "print(f\"  Median: {item_counts.median():.0f}\")\n",
        "\n",
        "# Simple visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(user_counts, bins=50, color='skyblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Interactions per User')\n",
        "axes[0].set_ylabel('Frequency (log scale)')\n",
        "axes[0].set_title(f'User Distribution ({len(user_counts):,} users)')\n",
        "axes[0].set_yscale('log')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(item_counts, bins=50, color='coral', edgecolor='black')\n",
        "axes[1].set_xlabel('Interactions per Item')\n",
        "axes[1].set_ylabel('Frequency (log scale)')\n",
        "axes[1].set_title(f'Item Distribution ({len(item_counts):,} items)')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('preprocessing_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Visualization saved: preprocessing_distribution.png\")\n"
      ],
      "metadata": {
        "id": "fqDlF1c5a0Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EWdC0L9UbuEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5 : Outlier Detection"
      ],
      "metadata": {
        "id": "UIppD9v7fgzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count interactions per user and item\n",
        "user_counts = inter_df['user_raw'].value_counts()\n",
        "item_counts = inter_df['item_raw'].value_counts()\n",
        "\n",
        "# Calculate quartiles (IQR method)\n",
        "user_q1 = user_counts.quantile(0.25)\n",
        "user_q3 = user_counts.quantile(0.75)\n",
        "user_iqr = user_q3 - user_q1\n",
        "user_upper = user_q3 + 1.5 * user_iqr\n",
        "\n",
        "item_q1 = item_counts.quantile(0.25)\n",
        "item_q3 = item_counts.quantile(0.75)\n",
        "item_iqr = item_q3 - item_q1\n",
        "item_upper = item_q3 + 1.5 * item_iqr\n",
        "\n",
        "# Find outliers\n",
        "user_outliers = user_counts[user_counts > user_upper]\n",
        "item_outliers = item_counts[item_counts > item_upper]\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nUSERS:\")\n",
        "print(f\"  Normal range: 0 - {user_upper:.0f} interactions\")\n",
        "print(f\"  Outliers (>{user_upper:.0f}): {len(user_outliers):,} users ({100*len(user_outliers)/len(user_counts):.1f}%)\")\n",
        "\n",
        "print(f\"\\nITEMS:\")\n",
        "print(f\"  Normal range: 0 - {item_upper:.0f} interactions\")\n",
        "print(f\"  Outliers (>{item_upper:.0f}): {len(item_outliers):,} items ({100*len(item_outliers)/len(item_counts):.1f}%)\")\n",
        "\n",
        "print(f\"\\nDECISION: Keep all data (no filtering)\")\n",
        "print(f\"  ✓ Leaderboard needs all users\")\n",
        "print(f\"  ✓ Outliers are real behavior\")\n",
        "\n",
        "# Simple visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# User boxplot\n",
        "axes[0].boxplot(user_counts, vert=True)\n",
        "axes[0].axhline(user_upper, color='red', linestyle='--', label=f'Threshold: {user_upper:.0f}')\n",
        "axes[0].set_ylabel('Interactions per User')\n",
        "axes[0].set_title(f'User Outliers\\n{len(user_outliers):,} outliers found')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Item boxplot\n",
        "axes[1].boxplot(item_counts, vert=True)\n",
        "axes[1].axhline(item_upper, color='red', linestyle='--', label=f'Threshold: {item_upper:.0f}')\n",
        "axes[1].set_ylabel('Interactions per Item')\n",
        "axes[1].set_title(f'Item Outliers\\n{len(item_outliers):,} outliers found')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outliers.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Saved: outliers.png\")\n"
      ],
      "metadata": {
        "id": "tk2GDvz4fHMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Step 6 : Map to Integer IDs"
      ],
      "metadata": {
        "id": "Y2xy88HLcJmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique entities\n",
        "unique_users = inter_df[\"user_raw\"].unique()\n",
        "unique_items = inter_df[\"item_raw\"].unique()\n",
        "\n",
        "# Create mappings\n",
        "user2id = {u: i for i, u in enumerate(unique_users)}\n",
        "item2id = {it: i for i, it in enumerate(unique_items)}\n",
        "\n",
        "num_users = len(user2id)\n",
        "num_items = len(item2id)\n",
        "\n",
        "# Apply mappings\n",
        "inter_df[\"user\"] = inter_df[\"user_raw\"].map(user2id)\n",
        "inter_df[\"item\"] = inter_df[\"item_raw\"].map(item2id)\n",
        "\n",
        "# Reverse mappings (for submission file)\n",
        "id2user_raw = {v: k for k, v in user2id.items()}\n",
        "id2item_raw = {v: k for k, v in item2id.items()}\n",
        "\n",
        "print(f\"\\n Mapped to integers:\")\n",
        "print(f\"  num_users = {num_users:,}\")\n",
        "print(f\"  num_items = {num_items:,}\")\n",
        "\n",
        "print(\"\\nSample:\")\n",
        "print(inter_df[[\"user_raw\", \"user\", \"item_raw\", \"item\"]].head())\n"
      ],
      "metadata": {
        "id": "BbPdsH-Sbxul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Mw_rL4yclfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 : Train/Validation Split"
      ],
      "metadata": {
        "id": "rzbAi1qfdHXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_df, val_df = train_test_split(\n",
        "    inter_df[[\"user\", \"item\"]],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\n Split complete:\")\n",
        "print(f\"  Train: {len(train_df):,} interactions (80%)\")\n",
        "print(f\"  Val: {len(val_df):,} interactions (20%)\")\n"
      ],
      "metadata": {
        "id": "J1-PiGqFcmJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 8 : Build Sparse Matrices"
      ],
      "metadata": {
        "id": "Jl1pKJwtjKgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_matrix(df, num_users, num_items):\n",
        "    user_idx = df[\"user\"].values\n",
        "    item_idx = df[\"item\"].values\n",
        "    data = np.ones(len(df), dtype=np.float32)\n",
        "\n",
        "    return sp.csr_matrix(\n",
        "        (data, (user_idx, item_idx)),\n",
        "        shape=(num_users, num_items),\n",
        "        dtype=np.float32\n",
        "    )\n",
        "\n",
        "# Build matrices\n",
        "train_matrix = build_matrix(train_df, num_users, num_items)\n",
        "val_matrix = build_matrix(val_df, num_users, num_items)\n",
        "full_matrix = build_matrix(inter_df[[\"user\", \"item\"]], num_users, num_items)\n",
        "\n",
        "print(f\"\\n Matrices created:\")\n",
        "print(f\"  Train: {train_matrix.shape}, {train_matrix.nnz:,} non-zero\")\n",
        "print(f\"  Val: {val_matrix.shape}, {val_matrix.nnz:,} non-zero\")\n",
        "print(f\"  Full: {full_matrix.shape}, {full_matrix.nnz:,} non-zero\")\n",
        "\n",
        "# Compute sparsity\n",
        "density = 100 * full_matrix.nnz / (num_users * num_items)\n",
        "print(f\"\\n  Sparsity: {100-density:.4f}%\")\n"
      ],
      "metadata": {
        "id": "2xXMJquvjEE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Step 9 : Visualization : Sparsity Analysis"
      ],
      "metadata": {
        "id": "C3c-DZSMmoVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 1. Sparsity bar chart\n",
        "axes[0].bar(['Dense\\n(observed)', 'Sparse\\n(missing)'],\n",
        "           [density, 100-density],\n",
        "           color=['#27ae60', '#e74c3c'],\n",
        "           edgecolor='black', linewidth=2)\n",
        "axes[0].set_ylabel('Percentage (%)')\n",
        "axes[0].set_title(f'Matrix Sparsity: {100-density:.4f}%')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add labels\n",
        "for i, val in enumerate([density, 100-density]):\n",
        "    label = f'{val:.6f}%' if val < 1 else f'{val:.1f}%'\n",
        "    axes[0].text(i, val + 2, label, ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# 2. Sample matrix pattern\n",
        "sample_size = 200\n",
        "matrix_sample = full_matrix[:sample_size, :sample_size].toarray()\n",
        "\n",
        "axes[1].imshow(matrix_sample, cmap='Blues', aspect='auto')\n",
        "axes[1].set_xlabel('Items (sample)')\n",
        "axes[1].set_ylabel('Users (sample)')\n",
        "axes[1].set_title(f'Matrix Pattern ({sample_size}×{sample_size} sample)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('preprocessing_sparsity.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Visualization saved: preprocessing_sparsity.png\")\n"
      ],
      "metadata": {
        "id": "3RUkuhqbmh5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10 : Build User-Item Lookup"
      ],
      "metadata": {
        "id": "KChXWgUxnHwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each user, store which items they've seen\n",
        "user_seen_items = {}\n",
        "\n",
        "for user_idx in range(num_users):\n",
        "    items = train_matrix[user_idx].indices\n",
        "    if len(items) > 0:\n",
        "        user_seen_items[user_idx] = set(items)\n",
        "\n",
        "print(f\"\\n Lookup dictionary created for {len(user_seen_items):,} users\")\n",
        "print(f\"\\nExample - User 0 has interacted with {len(user_seen_items.get(0, []))} items\")\n"
      ],
      "metadata": {
        "id": "xZ6xJ6gFnGaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11 : Item Popularity"
      ],
      "metadata": {
        "id": "MXhoK9riomAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top-20 popular items\n",
        "item_popularity = np.array(full_matrix.sum(axis=0)).flatten()\n",
        "popular_items = np.argsort(-item_popularity)[:20]\n",
        "\n",
        "print(f\"\\n Top-10 popular items:\")\n",
        "for i in range(10):\n",
        "    item_idx = popular_items[i]\n",
        "    count = int(item_popularity[item_idx])\n",
        "    print(f\"  {i+1}. Item {id2item_raw[item_idx]}: {count} interactions\")\n",
        "\n",
        "print(f\"\\n For cold-start users\")\n"
      ],
      "metadata": {
        "id": "TbJEmm3Nokfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zZjsbKECo6YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12 : Save Preprocessed Data"
      ],
      "metadata": {
        "id": "261GfzfmpEAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "os.makedirs('preprocessed', exist_ok=True)\n",
        "\n",
        "# Save mappings\n",
        "with open('preprocessed/user2id.pkl', 'wb') as f:\n",
        "    pickle.dump(user2id, f)\n",
        "with open('preprocessed/item2id.pkl', 'wb') as f:\n",
        "    pickle.dump(item2id, f)\n",
        "with open('preprocessed/id2user_raw.pkl', 'wb') as f:\n",
        "    pickle.dump(id2user_raw, f)\n",
        "with open('preprocessed/id2item_raw.pkl', 'wb') as f:\n",
        "    pickle.dump(id2item_raw, f)\n",
        "with open('preprocessed/user_seen_items.pkl', 'wb') as f:\n",
        "    pickle.dump(user_seen_items, f)\n",
        "with open('preprocessed/popular_items.pkl', 'wb') as f:\n",
        "    pickle.dump(popular_items, f)\n",
        "\n",
        "# Save matrices\n",
        "sp.save_npz('preprocessed/train_matrix.npz', train_matrix)\n",
        "sp.save_npz('preprocessed/val_matrix.npz', val_matrix)\n",
        "sp.save_npz('preprocessed/full_matrix.npz', full_matrix)\n",
        "\n",
        "# Save CSVs\n",
        "train_df.to_csv('preprocessed/train.csv', index=False)\n",
        "val_df.to_csv('preprocessed/val.csv', index=False)\n",
        "\n",
        "print(\"\\n Saved:\")\n",
        "print(\"  - 6 pickle files\")\n",
        "print(\"  - 3 sparse matrices\")\n",
        "print(\"  - 2 CSV files\")\n",
        "print(\"\\n All saved to 'preprocessed/' folder\")\n"
      ],
      "metadata": {
        "id": "7-27ln9ro7DD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}